{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d958cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and Set Constant Variables\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from array import *\n",
    "import csv\n",
    "import datetime;\n",
    "from bisect import bisect_left\n",
    "from bisect import bisect_right\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Queue\n",
    "from multiprocessing import current_process\n",
    "import queue\n",
    "import threading\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from community import modularity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c1ce57-facb-41bd-9372-3a829c11cffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481c483-a9cf-420e-bf73-809576d6286d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Define Base and Temporary Directory Paths\n",
    "\n",
    "\n",
    "BASE_ADDRESS = '/local/scratch/exported/Cardano_MCH_2023_1/'\n",
    "TEMP_ADDRESS = BASE_ADDRESS + '/temp_files/'\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a70a84-0b85-426c-adf1-82569661fc43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c26540b-e059-430f-afc3-f03f7ac60b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Define required methods:\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "def BinarySearch(a, x):\n",
    "    i = bisect_left(a, x)\n",
    "    if i < len(a) and a[i] == x:\n",
    "        return i\n",
    "    else:\n",
    "        print('BinarySearch Error: -1')\n",
    "        return -1\n",
    "\n",
    "##########################################################################################\n",
    "def BinarySearch_Find_start_end(a, x):\n",
    "    i = bisect_left(a, x)\n",
    "    j = bisect_right(a, x) - 1\n",
    "    if i < len(a) and a[i] == x and j < len(a) and a[j] == x:\n",
    "        return [i, j]\n",
    "    else:\n",
    "        print('BinarySearch Error: -1')\n",
    "        print('i = ', i)\n",
    "        print('j = ', j)\n",
    "        return -1\n",
    "\n",
    "##########################################################################################\n",
    "def store_array_to_file_2D (input_array_name, file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Store Array 2D to ' + file_name + '): ', ct)\n",
    "\n",
    "    with open(file_name, \"w\") as filehandle:\n",
    "        json.dump(input_array_name, filehandle)\n",
    "    \n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Store Array 2D to ' + file_name + '): ', et)\n",
    "\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def load_file_to_array_2D (file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Load ' + file_name  + ' to Array 2D): ', ct)\n",
    "\n",
    "    with open(file_name) as filehandle:\n",
    "        output_array_name = json.load(filehandle)\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Load ' + file_name  + ' to Array 2D): ', et)\n",
    "    \n",
    "    return output_array_name\n",
    "\n",
    "##########################################################################################\n",
    "def store_dict_to_file_INT (input_dict_name, file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Store Dictionary to ' + file_name + '): ', ct)\n",
    "\n",
    "    filehandle = csv.writer(open(file_name, 'w'))\n",
    "    for key, val in input_dict_name.items():\n",
    "        filehandle.writerow([key, val])\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Store Dictionary to ' + file_name + '): ', et)\n",
    "\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def load_file_to_dict_INT (file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Load ' + file_name  + ' to Dictionary): ', ct)\n",
    "\n",
    "    filehandle = csv.reader(open(file_name, 'r'))\n",
    "    output_dict_name = {int(rows[0]):int(rows[1]) for rows in filehandle}\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Load ' + file_name  + ' to Dictionary): ', et)\n",
    "    \n",
    "    return output_dict_name\n",
    "\n",
    "##########################################################################################\n",
    "def BinarySearch_Find_start_end(a, x):\n",
    "    i = bisect_left(a, x)\n",
    "    j = bisect_right(a, x) - 1\n",
    "    if i < len(a) and a[i] == x and j < len(a) and a[j] == x:\n",
    "        return [i, j]\n",
    "    else:\n",
    "        print('BinarySearch Error: -1')\n",
    "        print('i = ', i)\n",
    "        print('j = ', j)\n",
    "        return -1\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "def find_weights_graphEdges(graghEdges_array, graph_weights):\n",
    "    if (len(graghEdges_array) != len(graph_weights)):\n",
    "        print('find_weights_graphEdges Error: -1 (Length)')\n",
    "        return -1\n",
    "    \n",
    "    #for i in range(len(graghEdges_array)):\n",
    "    for i in tqdm(range(len(graghEdges_array))):\n",
    "        nodes = np.unique(graghEdges_array[i])\n",
    "        edges = graghEdges_array[i]\n",
    "        edges.sort()\n",
    "        \n",
    "        for j in nodes:\n",
    "            # (j,w)=weighted edge: j=node number, w=weight\n",
    "            x = BinarySearch_Find_start_end(edges, j)\n",
    "            w = x[1] - x[0] + 1\n",
    "            graph_weights[i].append((i,j,w)) \n",
    "        \n",
    "        #if (i % 1000000 == 0):\n",
    "        #    print('One milion records done!' , i)\n",
    "\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ba4cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd930a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read (\"sorted\" \"unique\" array_list) [raw_address_list/payment_address_list/delegation_address_list] from file:\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/Unique_AddressesListRaw__Cardano_TXs_All__2023-02-28_143357.txt'\n",
    "unique_raw_addresses = load_file_to_array (file_name)\n",
    "print('Length of \\\"unique_raw_addresses\\\" = ' + str(len(unique_raw_addresses)))\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/Unique_AddressesListPayment__Cardano_TXs_All__2023-02-28_143953.txt'\n",
    "unique_payment_addresses = load_file_to_array (file_name)\n",
    "print('Length of \\\"unique_payment_addresses\\\" = ' + str(len(unique_payment_addresses)))\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/Unique_AddressesListDelegation__Cardano_TXs_All__2023-02-28_144415.txt'\n",
    "unique_delegation_addresses = load_file_to_array (file_name)\n",
    "print('Length of \\\"unique_delegation_addresses\\\" = ' + str(len(unique_delegation_addresses)))\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "INITIAL_DATE_CARDANO      = datetime.datetime.strptime('2017-09-23 21:44:51', '%Y-%m-%d %H:%M:%S').date()\n",
    "FINAL_DATE_CARDANO        = datetime.datetime.strptime('2023-01-21 17:39:30', '%Y-%m-%d %H:%M:%S').date()\n",
    "total_time_length_CARDANO = int((FINAL_DATE_CARDANO - INITIAL_DATE_CARDANO).total_seconds()/86400) + 1\n",
    "\n",
    "unique_raw_addresses_len        = len(unique_raw_addresses)\n",
    "unique_payment_addresses_len    = len(unique_payment_addresses)\n",
    "unique_delegation_addresses_len = len(unique_delegation_addresses)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1863b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c3633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Address Clustering Results from Files:\n",
    "\n",
    "file_name = BASE_ADDRESS + '/clusteringArrayList_Heuristic1noSC__Cardano_TXs_All__2023-02-25_223957.txt'\n",
    "clustering_array_heur1 = load_file_to_array (file_name)\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/clusteringArrayList_Heuristic2__Cardano_TXs_All__2023-03-26_110150.txt'\n",
    "clustering_array_heur2 = load_file_to_array (file_name)\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/clusteringArrayList_Heuristic1noSC_AND_Heuristic2__Cardano_TXs_All__2023-03-26_141212.txt'\n",
    "clustering_array_heur1and2 = load_file_to_array (file_name)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd92b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2954a8-740e-4277-b11b-84c47c184cab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c954e0-516a-4677-b505-7f4f5adf4481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graphEdges_merged from file: \n",
    "\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "# it contains all edges of the address network graph.\n",
    "# \"graphEdges_merged\" contains all identified edges in all transactions.:\n",
    "\n",
    "\n",
    "graphEdges_merged = load_file_to_array_2D(BASE_ADDRESS + '/graphEdgesArrayList_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-25_224222.txt')\n",
    "\n",
    "print('Lenght of \\\"graphEdges_merged\\\" = ', len(graphEdges_merged))\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef67edb-f41d-4c79-a839-f0c3319a2465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8bac4-df9e-4649-9beb-b6eef74d7089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate graph_weights (everytime that two addresses are identified to be clustered together according to \n",
    "# a heuristic in a transaction, the weight of the edge between the two addresses is increased by 1):\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "graph_weights = [[] for _ in range(unique_addresses_len)]\n",
    "find_weights_graphEdges(graphEdges_merged, graph_weights)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Calculate graph_weights): \", et)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5da97-d942-4b57-941b-e96a72c56490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90c43ed-f7e9-4996-9ce2-0cb57908d660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store/Load \"graph_weights\" into/from file:\n",
    "\n",
    "\n",
    "\n",
    "# Store graph_weights into file:\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "ct = datetime.datetime.now()\n",
    "curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "output_filename = BASE_ADDRESS + '/graphWeightsArrayList_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__' + curr_timestamp + '.txt'\n",
    "print('output_filename = ', output_filename)\n",
    "\n",
    "with open(output_filename, 'w') as filehandle:\n",
    "        for element in graph_weights:\n",
    "            filehandle.write(f'{element}\\n')\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Store graph_weights into file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load graph_weights from file:\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/graphWeightsArrayList_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-25_234559.txt'\n",
    "print('input_filename = ', input_filename)\n",
    "\n",
    "\n",
    "graph_weights = [[] for _ in range(unique_addresses_len)]\n",
    "\n",
    "i=0\n",
    "with open(input_filename) as filehandle:\n",
    "    for row in filehandle:\n",
    "        graph_weights[i] = ast.literal_eval(row[:-1])\n",
    "        i = i+1\n",
    "        if (i % 1000000 == 0):\n",
    "            print('One milion records done!' , i)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Store graph_weights into file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06970d9-06f0-4c52-9a56-edbbed20d9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92a106-349f-4232-9835-e33cd76a98a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate \"Graph G Addrs network\" using graph_weights:\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for i in tqdm(range(len(graph_weights))):\n",
    "    G.add_node(i)\n",
    "    G.add_weighted_edges_from(graph_weights[i])\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "print('Is Connected    (G) = ', nx.is_connected(G))\n",
    "print('Number of Nodes (G) = ', G.number_of_nodes())\n",
    "print('Number of Edges (G) = ', G.number_of_edges())\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "test_results = [39057080, 454231, 397965]\n",
    "for i in test_results:\n",
    "    print('----------------------')\n",
    "    print('Degree of node ' + str(i) + ' = ', G.degree()[i])\n",
    "    print('graph_weights[' + str(i) + '] = ', graph_weights[i])\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Store graph_weights into file): \", et)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ee08fa-081e-4661-bfb8-8861f7f46bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c50db8-9fd2-402e-9ca3-5031ad438caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store/Load graph object \"Graph G Addrs network\" to/from file:\n",
    "\n",
    "\n",
    "# Store graph object to file:\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "import pickle\n",
    "curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "output_filename = BASE_ADDRESS + '/Graph_G_AddrsNetwork_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__' + curr_timestamp + '.pickle'\n",
    "print('output_filename = ', output_filename)\n",
    "pickle.dump(G, open(output_filename, 'wb'))\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Store G into file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load graph object from file:\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "import pickle\n",
    "input_filename = BASE_ADDRESS + '/Graph_G_AddrsNetwork_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_000507.pickle'\n",
    "print('input_filename = ', input_filename)\n",
    "G = pickle.load(open(input_filename, 'rb'))\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Load G from file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e896e-babe-4f3e-be38-55e72ba1f38b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980243e5-8917-4bcb-b4a3-0de397c13122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the largest connected component:\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "\n",
    "#largest_cc = max(nx.connected_components(G), key=len)\n",
    "largest_cc = list(sorted(nx.connected_components(G), key=len, reverse=True))[0]\n",
    "largest_cc_subgraph = G.subgraph(largest_cc).copy()\n",
    "\n",
    "print('Is Connected    (largest_cc_subgraph) = ', nx.is_connected(largest_cc_subgraph))\n",
    "print('Number of Nodes (largest_cc_subgraph) = ', largest_cc_subgraph.number_of_nodes())\n",
    "print('Number of Edges (largest_cc_subgraph) = ', largest_cc_subgraph.number_of_edges())\n",
    "\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "# Find weighted degrees of nodes:\n",
    "degree_sequence = sorted((d for n, d in largest_cc_subgraph.degree(weight='weight')), reverse=False)\n",
    "print('Sum Weighted Degrees (largest_cc_subgraph) = ', sum(degree_sequence))\n",
    "print('Max Weighted Degrees (largest_cc_subgraph) = ', max(degree_sequence))\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "for i in range(1, 15):\n",
    "    x = BinarySearch_Find_start_end(degree_sequence, i)\n",
    "    print('Number of nodes with Weighted Degree \\\"' + str(i) + '\\\" = ', x[1] - x[0] + 1)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Store graph_weights into file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da07fe-733c-4ae2-a75d-4771574a70cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d7591-e1f1-44dc-9990-ddb9dcc27258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store/Load graph object \"largest_cc_subgraph\" to/from file:\n",
    "\n",
    "\n",
    "# Store graph object to file:\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "import pickle\n",
    "curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "output_filename = BASE_ADDRESS + '/Largest1_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__' + curr_timestamp + '.pickle'\n",
    "print('output_filename = ', output_filename)\n",
    "pickle.dump(largest_cc_subgraph, open(output_filename, 'wb'))\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Store largest_cc_subgraph into file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load graph object from file:\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "import pickle\n",
    "input_filename = BASE_ADDRESS + '/Largest2_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-01-10_181953.pickle'\n",
    "print('input_filename = ', input_filename)\n",
    "largest_cc_subgraph = pickle.load(open(input_filename, 'rb'))\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Load largest_cc_subgraph into file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fc0752-8c54-4fcc-b561-39cb1f032858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f63de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688e7115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 9 superclusters (largest_cc_subgraphs):\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "input_filename = BASE_ADDRESS + '/Largest1_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_140703.pickle'\n",
    "largest_CCs_1 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_1 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest2_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_080139.pickle'\n",
    "largest_CCs_2 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_2 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest3_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_083614.pickle'\n",
    "largest_CCs_3 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_3 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest4_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_085210.pickle'\n",
    "largest_CCs_4 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_4 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest5_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_085920.pickle'\n",
    "largest_CCs_5 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_5 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest6_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_085927.pickle'\n",
    "largest_CCs_6 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_6 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest7_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_085927.pickle'\n",
    "largest_CCs_7 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_7 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest8_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_085927.pickle'\n",
    "largest_CCs_8 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_8 loaded!')\n",
    "\n",
    "input_filename = BASE_ADDRESS + '/Largest9_cc_subgraph_Heuristic1noSC_LinkToALLAddressesInTX__Cardano_TXs_All__2023-02-26_085927.pickle'\n",
    "largest_CCs_9 = pickle.load(open(input_filename, 'rb'))\n",
    "print('largest_CCs_9 loaded!')\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time: \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ecd75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT: degree distribution of 9 superclusters:\n",
    "\n",
    "plt.style.use('https://raw.githubusercontent.com/benckj/mpl_style/main/uzh.mplstyle')\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "# Find NoWeight degrees of nodes:\n",
    "noWeight_degree_seq_1 = sorted((d for n, d in tqdm(largest_CCs_1.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_1, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_1)),40))\n",
    "\n",
    "noWeight_degree_seq_2 = sorted((d for n, d in tqdm(largest_CCs_2.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_2, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_2)),40))\n",
    "\n",
    "noWeight_degree_seq_3 = sorted((d for n, d in tqdm(largest_CCs_3.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_3, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_3)),40))\n",
    "\n",
    "noWeight_degree_seq_4 = sorted((d for n, d in tqdm(largest_CCs_4.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_4, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_4)),40))\n",
    "\n",
    "noWeight_degree_seq_5 = sorted((d for n, d in tqdm(largest_CCs_5.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_5, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_5)),40))\n",
    "\n",
    "noWeight_degree_seq_6 = sorted((d for n, d in tqdm(largest_CCs_6.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_6, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_6)),40))\n",
    "\n",
    "noWeight_degree_seq_7 = sorted((d for n, d in tqdm(largest_CCs_7.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_7, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_7)),40))\n",
    "\n",
    "noWeight_degree_seq_8 = sorted((d for n, d in tqdm(largest_CCs_8.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_8, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_8)),40))\n",
    "\n",
    "noWeight_degree_seq_9 = sorted((d for n, d in tqdm(largest_CCs_9.degree())), reverse=True) # sort by NodeDegree_NoWeight\n",
    "plt.hist(noWeight_degree_seq_9, density=True, histtype='step', bins = np.logspace(np.log10(1), np.log10(max(noWeight_degree_seq_9)),40))\n",
    "\n",
    "\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Probability')\n",
    "\n",
    "plt.legend(['Supercluster 1', 'Supercluster 2', 'Supercluster 3', 'Supercluster 4', 'Supercluster 5', 'Supercluster 6', 'Supercluster 7', 'Supercluster 8', 'Supercluster 9'], fontsize=\"8\", loc =\"upper right\")\n",
    "\n",
    "plt.xscale(\"log\" )\n",
    "plt.yscale(\"log\" )\n",
    "\n",
    "plt.savefig('fig_deg_dist_super_clusters.pdf', bbox_inches='tight', facecolor='white')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time: \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea51230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e571d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT: power-law distributions (Superclusters degree distribution):\n",
    "\n",
    "\n",
    "fit_1 = powerlaw.Fit(noWeight_degree_seq_1)\n",
    "print('fit.power_law.alpha = ', fit_1.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_1.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_1.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_2 = powerlaw.Fit(noWeight_degree_seq_2)\n",
    "print('fit.power_law.alpha = ', fit_2.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_2.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_2.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_3 = powerlaw.Fit(noWeight_degree_seq_3)\n",
    "print('fit.power_law.alpha = ', fit_3.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_3.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_3.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_4 = powerlaw.Fit(noWeight_degree_seq_4)\n",
    "print('fit.power_law.alpha = ', fit_4.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_4.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_4.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_5 = powerlaw.Fit(noWeight_degree_seq_5)\n",
    "print('fit.power_law.alpha = ', fit_5.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_5.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_5.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_6 = powerlaw.Fit(noWeight_degree_seq_6)\n",
    "print('fit.power_law.alpha = ', fit_6.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_6.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_6.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_7 = powerlaw.Fit(noWeight_degree_seq_7)\n",
    "print('fit.power_law.alpha = ', fit_7.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_7.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_7.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_8 = powerlaw.Fit(noWeight_degree_seq_8)\n",
    "print('fit.power_law.alpha = ', fit_8.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_8.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_8.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "fit_9 = powerlaw.Fit(noWeight_degree_seq_9)\n",
    "print('fit.power_law.alpha = ', fit_9.power_law.alpha)\n",
    "print('fit.power_law.sigma = ', fit_9.power_law.sigma)\n",
    "print('fit.distribution_compare(\\'power_law\\', \\'exponential\\') = ', fit_9.distribution_compare('power_law', 'exponential'))\n",
    "\n",
    "\n",
    "###############################################################################################################################################\n",
    "plt.style.use('https://raw.githubusercontent.com/benckj/mpl_style/main/uzh.mplstyle')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Probability Density')\n",
    "\n",
    "plt.ylim(bottom=1e-12)\n",
    "\n",
    "###############################################################################################################################################\n",
    "markersize = 5\n",
    "\n",
    "fig = fit_1.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_1.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_2.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_2.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_3.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_3.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_4.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_4.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_5.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_5.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_6.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_6.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_7.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_7.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_8.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_8.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "fig = fit_9.plot_pdf(linestyle = '', marker='o', original_data=True, markersize=markersize)\n",
    "#fit_9.power_law.plot_pdf(linestyle = 'dashed', ax = fig)\n",
    "\n",
    "\n",
    "plt.legend(['Supercluster 1', 'Supercluster 2', 'Supercluster 3', 'Supercluster 4', 'Supercluster 5', 'Supercluster 6', 'Supercluster 7', 'Supercluster 8', 'Supercluster 9'], fontsize=\"10\", loc =\"upper right\")\n",
    "\n",
    "plt.savefig('fig_deg_dist_superclusters.pdf', bbox_inches='tight', facecolor='white')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc8f99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a7f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92292a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4312a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform \"Label Propagation\" on the largest_cc_subgraph_COPY (largest cluster):\n",
    "\n",
    "print('----------------------')\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    --rounds      INT    Number of iterations    Default is 8.\n",
    "    --seed        INT    Initial seed            Default is 42.\n",
    "    \"\"\"\n",
    "    args = {'rounds': 8, 'seed': 42}\n",
    "    model = LabelPropagator(largest_cc_subgraph_COPY, args)\n",
    "    NodeLabelsDict = model.do_a_series_of_propagations()\n",
    "    #NodeLabelsDict = dict(sorted(NodeLabelsDict.items()))                          # sorted by key\n",
    "    NodeLabelsDict = dict(sorted(NodeLabelsDict.items(), key=lambda item: item[1])) # sorted by value\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Store graph_weights into file): \", et)\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c466cf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12638d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbefc9d-a635-41be-9082-cab84672cef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
