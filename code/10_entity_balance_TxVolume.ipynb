{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343a3ce4-579d-4c90-89a9-c8dcceaa9116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "unique_raw_addresses_len        =  40330345\n",
      "unique_payment_addresses_len    =  40324960\n",
      "unique_delegation_addresses_len =  3868049\n",
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from array import *\n",
    "import csv\n",
    "\n",
    "# using datetime module\n",
    "import datetime;\n",
    "\n",
    "# Binary Search\n",
    "from bisect import bisect_left\n",
    "from bisect import bisect_right\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Queue\n",
    "from multiprocessing import current_process\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "#https://python-louvain.readthedocs.io/en/latest/api.html\n",
    "#community.modularity(partition, graph, weight='weight')\n",
    "from community import modularity\n",
    "\n",
    "import pickle\n",
    "\n",
    "import powerlaw\n",
    "\n",
    "print('----------------------')\n",
    "#unique_payment_addresses_len = len(unique_payment_addresses)\n",
    "unique_raw_addresses_len        = 40330345\n",
    "unique_payment_addresses_len    = 40324960\n",
    "unique_delegation_addresses_len = 3868049\n",
    "print('unique_raw_addresses_len        = ', unique_raw_addresses_len)\n",
    "print('unique_payment_addresses_len    = ', unique_payment_addresses_len)\n",
    "print('unique_delegation_addresses_len = ', unique_delegation_addresses_len)\n",
    "\n",
    "INITIAL_DATE_CARDANO      = datetime.datetime.strptime('2017-09-23 21:44:51', '%Y-%m-%d %H:%M:%S').date()\n",
    "FINAL_DATE_CARDANO        = datetime.datetime.strptime('2023-01-21 17:39:30', '%Y-%m-%d %H:%M:%S').date()\n",
    "total_time_length_CARDANO = int((FINAL_DATE_CARDANO - INITIAL_DATE_CARDANO).total_seconds()/86400) + 1\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126870a7-174d-4743-9a93-67b4d5e0cb9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fbba725-2eb7-41cd-827b-c646e087dd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "print('----------------------')\n",
    "\n",
    "#print(os.path.basename(\"/path/to/some/file.txt\"))\n",
    "#print(os.path.dirname(\"/path/to/some/file.txt\"))\n",
    "\n",
    "BASE_ADDRESS = '/local/scratch/exported/Cardano_MCH_2023_1/'\n",
    "TEMP_ADDRESS = BASE_ADDRESS + '/temp_files/'\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9a99b1-f892-492a-aa7b-62e0863d35a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "419c6f49-9bab-4893-949d-9921b3eb90c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "current time:  2024-02-17 17:08:11.092924\n",
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Define required methods:\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "def parent (id1, parents_array):\n",
    "    return parents_array[id1];\n",
    "\n",
    "##########################################################################################\n",
    "def find_parent (id1, parents_array):\n",
    "    while (id1 != parent(id1, parents_array)):\n",
    "        new_parent = parent(parent(id1, parents_array), parents_array);\n",
    "        id1        = new_parent;\n",
    "    \n",
    "    return id1\n",
    "\n",
    "##########################################################################################\n",
    "# Link two addresses based on \"Union-Find\" Algorithm:\n",
    "def link_address (addr_position_1, addr_position_2, parents_array):\n",
    "    id1 = find_parent(addr_position_1, parents_array)\n",
    "    id2 = find_parent(addr_position_2, parents_array)\n",
    "\n",
    "    if (id1 == id2):\n",
    "        return\n",
    "\n",
    "    # make sure always we have id1 > id2\n",
    "    if id1 < id2:\n",
    "        x = id1\n",
    "        id1 = id2\n",
    "        id2 = x\n",
    "\n",
    "    parents_array[id1] = id2\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def resolveAll (parents_array):\n",
    "    for id1 in tqdm(range(len(parents_array))):\n",
    "        parents_array[id1] = find_parent(id1, parents_array);\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def remapClusterIds (parents_array, clustering_array):\n",
    "    cluster_count = 0\n",
    "    place_holder = 9999999999999\n",
    "    new_cluster_ids = [place_holder] * len(parents_array)\n",
    "    \n",
    "    for i in range(len(clustering_array)):\n",
    "        clustering_array[i] = parents_array[i]\n",
    "\n",
    "    for i in tqdm(range(len(clustering_array))):\n",
    "        parent_index = clustering_array[i]\n",
    "\n",
    "        if (new_cluster_ids [parent_index] == place_holder):\n",
    "            new_cluster_ids [parent_index] = cluster_count\n",
    "            cluster_count = cluster_count + 1\n",
    "\n",
    "        clustering_array[i] = new_cluster_ids [parent_index]\n",
    "\n",
    "    return cluster_count;\n",
    "\n",
    "##########################################################################################\n",
    "def merge_parents(parents_array, parents_merged):\n",
    "    if (len(parents_array) != len(parents_merged)):\n",
    "        print('parents_merged Error: -1 (Length)')\n",
    "        return -1\n",
    "    \n",
    "    for i in tqdm(range(len(parents_merged))):\n",
    "        link_address (i, parents_array[i], parents_merged)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "def BinarySearch(a, x):\n",
    "    i = bisect_left(a, x)\n",
    "    if i < len(a) and a[i] == x:\n",
    "        return i\n",
    "    else:\n",
    "        print('BinarySearch Error: -1')\n",
    "        return -1\n",
    "\n",
    "##########################################################################################\n",
    "def BinarySearch_Find_start_end(a, x):\n",
    "    i = bisect_left(a, x)\n",
    "    j = bisect_right(a, x) - 1\n",
    "    if i < len(a) and a[i] == x and j < len(a) and a[j] == x:\n",
    "        return [i, j]\n",
    "    else:\n",
    "        print('BinarySearch Error: -1')\n",
    "        print('i = ', i)\n",
    "        print('j = ', j)\n",
    "        return -1\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "def store_array_to_file (input_array_name, file_name, index_=False, header_=None):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Store Array to ' + file_name + '): ', ct)\n",
    "\n",
    "    df = pd.DataFrame(input_array_name)\n",
    "    df.to_csv(file_name, index=index_, header=header_)\n",
    "\n",
    "    '''\n",
    "    with open(file_name, 'w') as filehandle:\n",
    "        for element in input_array_name:\n",
    "            filehandle.write(f'{element}\\n')\n",
    "    '''\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Store Array to ' + file_name + '): ', et)\n",
    "\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def load_file_to_array (file_name, header_=None):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Load ' + file_name  + ' to Array): ', ct)\n",
    "\n",
    "    df = pd.read_csv(file_name, header=header_)\n",
    "    output_array_name = df.to_numpy()\n",
    "    \n",
    "    '''\n",
    "    output_array_name = []\n",
    "    with open(file_name, 'r') as filehandle:\n",
    "        for line in filehandle:\n",
    "            # Remove linebreak which is the last character of the string\n",
    "            curr_place = line[:-1]\n",
    "            # Add item to the list\n",
    "            output_array_name.append(curr_place)\n",
    "    '''    \n",
    "\n",
    "    '''\n",
    "    for index, row in df.iterrows():\n",
    "        new_line = df.loc[index , 0]\n",
    "        output_array_name.append(new_line)\n",
    "        if (index%1000000 == 0):\n",
    "                print('One New Milion Records loaded: ', index)\n",
    "    '''\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Load ' + file_name  + ' to Array): ', et)\n",
    "    \n",
    "    return output_array_name\n",
    "\n",
    "##########################################################################################\n",
    "def store_array_to_file_2D (input_array_name, file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Store Array 2D to ' + file_name + '): ', ct)\n",
    "\n",
    "    with open(file_name, \"w\") as filehandle:\n",
    "        json.dump(input_array_name, filehandle)\n",
    "    \n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Store Array 2D to ' + file_name + '): ', et)\n",
    "\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def load_file_to_array_2D (file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Load ' + file_name  + ' to Array 2D): ', ct)\n",
    "\n",
    "    with open(file_name) as filehandle:\n",
    "        output_array_name = json.load(filehandle)\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Load ' + file_name  + ' to Array 2D): ', et)\n",
    "    \n",
    "    return output_array_name\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "def store_dict_to_file_INT (input_dict_name, file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Store Dictionary to ' + file_name + '): ', ct)\n",
    "\n",
    "    filehandle = csv.writer(open(file_name, 'w'))\n",
    "    for key, val in input_dict_name.items():\n",
    "        filehandle.writerow([key, val])\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Store Dictionary to ' + file_name + '): ', et)\n",
    "\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def load_file_to_dict_INT (file_name):\n",
    "    ct = datetime.datetime.now()\n",
    "    curr_timestamp = str(ct)[0:10] + '_' + str(ct)[11:13] + str(ct)[14:16] + str(ct)[17:19]\n",
    "    print('start time (Load ' + file_name  + ' to Dictionary): ', ct)\n",
    "\n",
    "    filehandle = csv.reader(open(file_name, 'r'))\n",
    "    output_dict_name = {int(rows[0]):int(rows[1]) for rows in filehandle}\n",
    "\n",
    "    et = datetime.datetime.now() - ct\n",
    "    print('elapsed time (Load ' + file_name  + ' to Dictionary): ', et)\n",
    "    \n",
    "    return output_dict_name\n",
    "\n",
    "##########################################################################################\n",
    "def add_edge_info(node_1, node_2, edges_array, weight=1):\n",
    "    if (node_1 == node_2):\n",
    "        return\n",
    "\n",
    "    # make sure always we have n1 > n2:\n",
    "    if (node_1 < node_2):\n",
    "        n1 = node_2\n",
    "        n2 = node_1\n",
    "    else:\n",
    "        n1 = node_1\n",
    "        n2 = node_2\n",
    "\n",
    "    for i in range(weight):\n",
    "        edges_array[n1].append(n2)\n",
    "\n",
    "    return\n",
    "\n",
    "##########################################################################################\n",
    "def extract_payment_delegation_parts(address_raw, payment_cred, stake_address):\n",
    "    if (address_raw == ''):\n",
    "        #print(' - Error: address_raw is empty!')\n",
    "        return ['', '']\n",
    "\n",
    "    if (address_raw[2] == '8'): #Byron Address\n",
    "        if (payment_cred != ''):\n",
    "            print(' - Error: payment_cred in Byron Address is NOT empty!')\n",
    "            return ['', '']\n",
    "        if (stake_address != ''):\n",
    "            print(' - Error: stake_address in Byron Address is NOT empty!')\n",
    "            return ['', '']\n",
    "        payment_part    = address_raw\n",
    "        delegation_part = ''\n",
    "\n",
    "    else: #Shelley Address\n",
    "        if (payment_cred == ''):\n",
    "            print(' - Error: payment_cred in Shelley Address is empty!')\n",
    "            return ['', '']\n",
    "        payment_part    = payment_cred\n",
    "        delegation_part = stake_address\n",
    "\n",
    "    return [payment_part, delegation_part]\n",
    "\n",
    "##########################################################################################\n",
    "# Function which calculates the Gini index\n",
    "# Inputs: array with length equal to number of agents; each element in array represents the wealth of the agent\n",
    "# Outputs: gini index of the system\n",
    "def gini_index(inp_array):\n",
    "    array = np.array(inp_array)\n",
    "    array = array.astype(float)\n",
    "    array = array.flatten() #all values are treated equally, arrays must be 1d\n",
    "    if np.amin(array) < 0:\n",
    "        array -= np.amin(array) #values cannot be negative\n",
    "    array += 0.0000001 #values cannot be 0\n",
    "    array = np.sort(array) #values must be sorted\n",
    "    index = np.arange(1,array.shape[0]+1) #index per array element\n",
    "    n = array.shape[0] #number of array elements\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array))) #Gini coefficient\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af409d67-9aa9-46b5-a82d-8c6c32a49b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e093eaf-dea4-498d-b56c-fb750766cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "start time (Load /local/scratch/exported/Cardano_MCH_2023_1//Unique_AddressesListRaw__Cardano_TXs_All__2023-02-28_143357.txt to Array):  2024-02-17 17:08:14.520066\n",
      "elapsed time (Load /local/scratch/exported/Cardano_MCH_2023_1//Unique_AddressesListRaw__Cardano_TXs_All__2023-02-28_143357.txt to Array):  0:00:47.484417\n",
      "Length of \"unique_raw_addresses\" = 40330345\n",
      "start time (Load /local/scratch/exported/Cardano_MCH_2023_1//Unique_AddressesListPayment__Cardano_TXs_All__2023-02-28_143953.txt to Array):  2024-02-17 17:09:02.005003\n",
      "elapsed time (Load /local/scratch/exported/Cardano_MCH_2023_1//Unique_AddressesListPayment__Cardano_TXs_All__2023-02-28_143953.txt to Array):  0:00:36.294476\n",
      "Length of \"unique_payment_addresses\" = 40324960\n",
      "start time (Load /local/scratch/exported/Cardano_MCH_2023_1//Unique_AddressesListDelegation__Cardano_TXs_All__2023-02-28_144415.txt to Array):  2024-02-17 17:09:38.299947\n",
      "elapsed time (Load /local/scratch/exported/Cardano_MCH_2023_1//Unique_AddressesListDelegation__Cardano_TXs_All__2023-02-28_144415.txt to Array):  0:00:02.796047\n",
      "Length of \"unique_delegation_addresses\" = 3868049\n",
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Read (\"sorted\" \"unique\" array_list) [raw_address_list/payment_address_list/delegation_address_list] from file:\n",
    "\n",
    "print('----------------------')\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/Unique_AddressesListRaw__Cardano_TXs_All__2023-02-28_143357.txt'\n",
    "unique_raw_addresses = load_file_to_array (file_name)\n",
    "print('Length of \\\"unique_raw_addresses\\\" = ' + str(len(unique_raw_addresses)))\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/Unique_AddressesListPayment__Cardano_TXs_All__2023-02-28_143953.txt'\n",
    "unique_payment_addresses = load_file_to_array (file_name)\n",
    "print('Length of \\\"unique_payment_addresses\\\" = ' + str(len(unique_payment_addresses)))\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/Unique_AddressesListDelegation__Cardano_TXs_All__2023-02-28_144415.txt'\n",
    "unique_delegation_addresses = load_file_to_array (file_name)\n",
    "print('Length of \\\"unique_delegation_addresses\\\" = ' + str(len(unique_delegation_addresses)))\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062658e-83a1-4704-bc05-32b82ec55b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69d56751-0ecb-41ff-814f-bb91b3a7dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start time (Load /local/scratch/exported/Cardano_MCH_2023_1//clusteringArrayList_Heuristic1noSC__Cardano_TXs_All__2023-02-25_223957.txt to Array):  2024-02-17 17:09:41.101550\n",
      "elapsed time (Load /local/scratch/exported/Cardano_MCH_2023_1//clusteringArrayList_Heuristic1noSC__Cardano_TXs_All__2023-02-25_223957.txt to Array):  0:00:02.703545\n",
      "start time (Load /local/scratch/exported/Cardano_MCH_2023_1//clusteringArrayList_Heuristic2__Cardano_TXs_All__2023-03-26_110150.txt to Array):  2024-02-17 17:09:43.805302\n",
      "elapsed time (Load /local/scratch/exported/Cardano_MCH_2023_1//clusteringArrayList_Heuristic2__Cardano_TXs_All__2023-03-26_110150.txt to Array):  0:00:02.702376\n",
      "start time (Load /local/scratch/exported/Cardano_MCH_2023_1//clusteringArrayList_Heuristic1noSC_AND_Heuristic2__Cardano_TXs_All__2023-03-26_141212.txt to Array):  2024-02-17 17:09:46.507906\n",
      "elapsed time (Load /local/scratch/exported/Cardano_MCH_2023_1//clusteringArrayList_Heuristic1noSC_AND_Heuristic2__Cardano_TXs_All__2023-03-26_141212.txt to Array):  0:00:02.493574\n",
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Read clustering_array[] from file:\n",
    "\n",
    "file_name = BASE_ADDRESS + '/clusteringArrayList_Heuristic1noSC__Cardano_TXs_All__2023-02-25_223957.txt'\n",
    "clustering_array_heur1 = load_file_to_array (file_name)\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/clusteringArrayList_Heuristic2__Cardano_TXs_All__2023-03-26_110150.txt'\n",
    "clustering_array_heur2 = load_file_to_array (file_name)\n",
    "\n",
    "\n",
    "file_name = BASE_ADDRESS + '/clusteringArrayList_Heuristic1noSC_AND_Heuristic2__Cardano_TXs_All__2023-03-26_141212.txt'\n",
    "clustering_array_heur1and2 = load_file_to_array (file_name)\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4500bce-d998-4142-9164-9759eb30b8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a984da-8f82-422e-921b-bcd5a4f699bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "\"unique_raw_addresses_len\"             =  40330345\n",
      "\"unique_payment_addresses_len\"         =  40324960\n",
      "\"unique_delegation_addresses_len\"      =  3868049\n",
      "Length of \"clustering_array_heur1\"     =  40324960\n",
      "number of clusters_heur1               =  19249106\n",
      "Length of \"clustering_array_heur2\"     =  40324960\n",
      "number of clusters_heur2               =  18529342\n",
      "Length of \"clustering_array_heur1and2\" =  40324960\n",
      "number of clusters_heur1and2           =  8805791\n",
      "----------------------\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('\\\"unique_raw_addresses_len\\\"             = ', unique_raw_addresses_len)\n",
    "print('\\\"unique_payment_addresses_len\\\"         = ', unique_payment_addresses_len)\n",
    "print('\\\"unique_delegation_addresses_len\\\"      = ', unique_delegation_addresses_len)\n",
    "\n",
    "print('Length of \\\"clustering_array_heur1\\\"     = ', len(clustering_array_heur1))\n",
    "print('number of clusters_heur1               = ', len(np.unique(clustering_array_heur1)))\n",
    "print('Length of \\\"clustering_array_heur2\\\"     = ', len(clustering_array_heur2))\n",
    "print('number of clusters_heur2               = ', len(np.unique(clustering_array_heur2)))\n",
    "print('Length of \\\"clustering_array_heur1and2\\\" = ', len(clustering_array_heur1and2))\n",
    "print('number of clusters_heur1and2           = ', len(np.unique(clustering_array_heur1and2)))\n",
    "\n",
    "\n",
    "##########################################################################################\n",
    "print('----------------------')\n",
    "print('done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0575ff-5b30-4a0c-8533-f5a5c642a328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5346798-cbda-4193-912a-bf945ac1cfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53ba38-e674-437a-a2e4-c78144130f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde527a9-e884-4093-b2e8-796178ac212b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb063893-6f80-42dd-b086-51b44eab9d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfbc898-6dfc-40e7-8fd8-2fedc899d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Entities Balances:\n",
    "\n",
    "'''\n",
    "print('----------------------')\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "print('----------------------')\n",
    "\n",
    "\n",
    "INITIAL_DATE_CARDANO      = datetime.datetime.strptime('2017-09-23 21:44:51', '%Y-%m-%d %H:%M:%S').date()\n",
    "FINAL_DATE_CARDANO        = datetime.datetime.strptime('2023-01-21 17:39:30', '%Y-%m-%d %H:%M:%S').date()\n",
    "total_time_length_CARDANO = int((FINAL_DATE_CARDANO - INITIAL_DATE_CARDANO).total_seconds()/86400) + 1\n",
    "\n",
    "\n",
    "\n",
    "# Choose from: \"clustering_array_heur1\"  OR  \"clustering_array_heur2\"  OR  \"clustering_array_heur1and2\"\n",
    "clustering_array = clustering_array_heur1and2\n",
    "\n",
    "balances_per_entity_array = [0] * ( np.amax(clustering_array)+1 )\n",
    "current_delta_day = 0\n",
    "\n",
    "\n",
    "\n",
    "CSV_FILES_NAME_FORMAT = BASE_ADDRESS + '/cardano_TXs_Velocity_'\n",
    "NUMBER_OF_CSV_FILES = 6\n",
    "CSV_FILES_SUFFIX = '.csv'\n",
    "\n",
    "\n",
    "for i in range(1, NUMBER_OF_CSV_FILES + 1):\n",
    "\n",
    "    ct_temp = datetime.datetime.now()\n",
    "\n",
    "    file_name = CSV_FILES_NAME_FORMAT + str(i) + CSV_FILES_SUFFIX\n",
    "    df = pd.read_csv(file_name, delimiter='|')\n",
    "\n",
    "    et_temp = datetime.datetime.now() - ct_temp\n",
    "    print(\"elapsed time (Load CSV File \" + file_name + \"): \", et_temp)\n",
    "\n",
    "    ct_temp = datetime.datetime.now()\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        ##########################################################################################\n",
    "        TX_ID      = df.loc[index , 'TX_ID']\n",
    "        ##########################################################################################\n",
    "        BLOCK_TIME = datetime.datetime.strptime(str(df.loc[index , 'BLOCK_TIME']), '%Y-%m-%d %H:%M:%S').date()\n",
    "        tx_delta_day = int((BLOCK_TIME - INITIAL_DATE_CARDANO).total_seconds()/86400)\n",
    "        if(current_delta_day < tx_delta_day):\n",
    "            output_filename = BASE_ADDRESS + '/YuZhang_Cardano_Balances_Entities/YuZhang__BalancesPerEntityDay_' + str(current_delta_day).zfill(4) + '__Cardano_TXs_All.txt'\n",
    "            #print('output_filename = ', output_filename)\n",
    "            #pickle.dump(balances_per_entity_array, open(output_filename, 'wb'))\n",
    "            store_array_to_file(balances_per_entity_array, output_filename)\n",
    "            current_delta_day = tx_delta_day\n",
    "\n",
    "        ##########################################################################################\n",
    "        EPOCH_NO   = str( df.loc[index , 'EPOCH_NO'] )\n",
    "        ##########################################################################################\n",
    "        inputs_list = list( df.loc[index , 'INPUTs'].split(';') )\n",
    "        #for tx_input in inputs_list:\n",
    "        #    address_raw           = tx_input.split(',')[4]\n",
    "        #    address_has_script    = tx_input.split(',')[7]\n",
    "        #    payment_cred          = tx_input.split(',')[8]\n",
    "        #    stake_address         = tx_input.split(',')[9]\n",
    "        #    input_value           = int(tx_input.split(',')[6])\n",
    "        #    input_time_str        = tx_input.split(',')[10]\n",
    "        #    INPUT_TIME            = datetime.datetime.strptime(input_time_str, '%Y-%m-%d %H:%M:%S').date()\n",
    "        #    INPUT_HOLDING_DAY     = int((BLOCK_TIME - INPUT_TIME).total_seconds()/86400)\n",
    "        #    \n",
    "        #    # Generate a random float between 0 and 1\n",
    "        #    random_float = random.random()\n",
    "        #    if(random_float <= 1):\n",
    "        #        hodling_day_array[INPUT_HOLDING_DAY] = hodling_day_array[INPUT_HOLDING_DAY] + input_value\n",
    "        ##########################################################################################\n",
    "        outputs_list = list( df.loc[index , 'OUTPUTs'].split(';') )\n",
    "        #for tx_output in outputs_list:\n",
    "        #    address_raw        = tx_output.split(',')[1]\n",
    "        #    address_has_script = tx_output.split(',')[4]\n",
    "        #    payment_cred       = tx_output.split(',')[5]\n",
    "        #    stake_address      = tx_output.split(',')[6]\n",
    "        #    [address_payment_part, address_delegation_part] = extract_payment_delegation_parts(address_raw, payment_cred, stake_address)\n",
    "        #    if (address_payment_part != '' and address_delegation_part != ''):\n",
    "        #        indx1 = BinarySearch(unique_delegation_addresses, address_delegation_part)\n",
    "        #        indx2 = BinarySearch(unique_payment_addresses, address_payment_part)\n",
    "        #        stake_delegation_array[indx1].append(indx2)\n",
    "        ##########################################################################################\n",
    "        # Update all ADA balances:\n",
    "        for i in range(0, len(inputs_list)):\n",
    "            address_has_script = inputs_list[i].split(',')[7]\n",
    "            if (address_has_script == 'f'): # non-Smart Contract Address\n",
    "                address_raw   = inputs_list[i].split(',')[4]\n",
    "                payment_cred  = inputs_list[i].split(',')[8]\n",
    "                stake_address = inputs_list[i].split(',')[9]\n",
    "                [address_payment_part, address_delegation_part] = extract_payment_delegation_parts(address_raw, payment_cred, stake_address)\n",
    "                if (address_payment_part != ''):\n",
    "                    addr_indx   = BinarySearch(unique_payment_addresses, address_payment_part)\n",
    "                    entity_indx = clustering_array[addr_indx][0]\n",
    "                    UTXO_value  = int(inputs_list[i].split(',')[6])\n",
    "                    balances_per_entity_array[entity_indx] = balances_per_entity_array[entity_indx] - int(UTXO_value)\n",
    "\n",
    "        for i in range(0, len(outputs_list)):\n",
    "            address_has_script = outputs_list[i].split(',')[4]\n",
    "            if (address_has_script == 'f'): # non-Smart Contract Address\n",
    "                address_raw   = outputs_list[i].split(',')[1]\n",
    "                payment_cred  = outputs_list[i].split(',')[5]\n",
    "                stake_address = outputs_list[i].split(',')[6]\n",
    "                [address_payment_part, address_delegation_part] = extract_payment_delegation_parts(address_raw, payment_cred, stake_address)\n",
    "                if (address_payment_part != ''):\n",
    "                    addr_indx   = BinarySearch(unique_payment_addresses, address_payment_part)\n",
    "                    entity_indx = clustering_array[addr_indx][0]\n",
    "                    UTXO_value  = int(outputs_list[i].split(',')[3])\n",
    "                    balances_per_entity_array[entity_indx] = balances_per_entity_array[entity_indx] + int(UTXO_value)\n",
    "\n",
    "        ##########################################################################################\n",
    "\n",
    "    et_temp = datetime.datetime.now() - ct_temp\n",
    "    print(\"elapsed time (ADA Velocity from CSV File \" + file_name + \"): \", et_temp)\n",
    "\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (ADA Velocity): \", et)\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05e1bd9-e349-4f58-a60e-d93718b6de37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93da9f-9a6f-4956-b58a-451ecb908251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b9ba2-dd2e-40a3-a5db-6d8f9e835961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c68ccd-76c1-4aea-961c-486cde81f547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "current time:  2024-02-17 18:44:27.006859\n",
      "----------------------\n",
      "elapsed time (Load CSV File /local/scratch/exported/Cardano_MCH_2023_1//cardano_TXs_Velocity_1.csv):  0:01:19.366201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000000it [1:35:05, 1752.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time (Entities TX_vol from CSV File /local/scratch/exported/Cardano_MCH_2023_1//cardano_TXs_Velocity_1.csv):  1:35:05.693770\n",
      "elapsed time (Load CSV File /local/scratch/exported/Cardano_MCH_2023_1//cardano_TXs_Velocity_2.csv):  0:01:23.925796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000000it [1:30:42, 1837.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time (Entities TX_vol from CSV File /local/scratch/exported/Cardano_MCH_2023_1//cardano_TXs_Velocity_2.csv):  1:30:42.901717\n",
      "elapsed time (Load CSV File /local/scratch/exported/Cardano_MCH_2023_1//cardano_TXs_Velocity_3.csv):  0:01:38.098220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8664589it [1:24:06, 1793.86it/s]"
     ]
    }
   ],
   "source": [
    "# Calculate Entities TX_vol:\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "# ct stores current time\n",
    "ct = datetime.datetime.now()\n",
    "print(\"current time: \", ct)\n",
    "print('----------------------')\n",
    "\n",
    "\n",
    "INITIAL_DATE_CARDANO      = datetime.datetime.strptime('2017-09-23 21:44:51', '%Y-%m-%d %H:%M:%S').date()\n",
    "FINAL_DATE_CARDANO        = datetime.datetime.strptime('2023-01-21 17:39:30', '%Y-%m-%d %H:%M:%S').date()\n",
    "total_time_length_CARDANO = int((FINAL_DATE_CARDANO - INITIAL_DATE_CARDANO).total_seconds()/86400) + 1\n",
    "\n",
    "\n",
    "\n",
    "# Choose from: \"clustering_array_heur1\"  OR  \"clustering_array_heur2\"  OR  \"clustering_array_heur1and2\"\n",
    "clustering_array = clustering_array_heur1and2\n",
    "\n",
    "TX_vol_per_entity_array = [0] * ( np.amax(clustering_array)+1 )\n",
    "current_delta_day = 0\n",
    "\n",
    "\n",
    "\n",
    "CSV_FILES_NAME_FORMAT = BASE_ADDRESS + '/cardano_TXs_Velocity_'\n",
    "NUMBER_OF_CSV_FILES = 6\n",
    "CSV_FILES_SUFFIX = '.csv'\n",
    "\n",
    "\n",
    "for i in range(1, NUMBER_OF_CSV_FILES + 1):\n",
    "\n",
    "    ct_temp = datetime.datetime.now()\n",
    "\n",
    "    file_name = CSV_FILES_NAME_FORMAT + str(i) + CSV_FILES_SUFFIX\n",
    "    df = pd.read_csv(file_name, delimiter='|')\n",
    "\n",
    "    et_temp = datetime.datetime.now() - ct_temp\n",
    "    print(\"elapsed time (Load CSV File \" + file_name + \"): \", et_temp)\n",
    "\n",
    "    ct_temp = datetime.datetime.now()\n",
    "\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        ##########################################################################################\n",
    "        TX_ID      = df.loc[index , 'TX_ID']\n",
    "        ##########################################################################################\n",
    "        BLOCK_TIME = datetime.datetime.strptime(str(df.loc[index , 'BLOCK_TIME']), '%Y-%m-%d %H:%M:%S').date()\n",
    "        tx_delta_day = int((BLOCK_TIME - INITIAL_DATE_CARDANO).total_seconds()/86400)\n",
    "        \n",
    "        if(current_delta_day < tx_delta_day):\n",
    "            output_filename = BASE_ADDRESS + '/YuZhang_Cardano_TX_Vols_Entities__PICKLE/TX_Vol_PerEntityDay_' + str(current_delta_day).zfill(4) + '__Cardano_TXs_All.pickle'\n",
    "            #print('output_filename = ', output_filename)\n",
    "\n",
    "            pickle.dump(TX_vol_per_entity_array, open(output_filename, 'wb'))\n",
    "            #store_array_to_file(TX_vol_per_entity_array, output_filename)\n",
    "            \n",
    "            TX_vol_per_entity_array = [0] * ( np.amax(clustering_array)+1 )\n",
    "\n",
    "            current_delta_day = tx_delta_day\n",
    "\n",
    "        ##########################################################################################\n",
    "        EPOCH_NO   = str( df.loc[index , 'EPOCH_NO'] )\n",
    "        ##########################################################################################\n",
    "        inputs_list = list( df.loc[index , 'INPUTs'].split(';') )\n",
    "        #for tx_input in inputs_list:\n",
    "        #    address_raw           = tx_input.split(',')[4]\n",
    "        #    address_has_script    = tx_input.split(',')[7]\n",
    "        #    payment_cred          = tx_input.split(',')[8]\n",
    "        #    stake_address         = tx_input.split(',')[9]\n",
    "        #    input_value           = int(tx_input.split(',')[6])\n",
    "        #    input_time_str        = tx_input.split(',')[10]\n",
    "        #    INPUT_TIME            = datetime.datetime.strptime(input_time_str, '%Y-%m-%d %H:%M:%S').date()\n",
    "        #    INPUT_HOLDING_DAY     = int((BLOCK_TIME - INPUT_TIME).total_seconds()/86400)\n",
    "        #    \n",
    "        #    # Generate a random float between 0 and 1\n",
    "        #    random_float = random.random()\n",
    "        #    if(random_float <= 1):\n",
    "        #        hodling_day_array[INPUT_HOLDING_DAY] = hodling_day_array[INPUT_HOLDING_DAY] + input_value\n",
    "        ##########################################################################################\n",
    "        outputs_list = list( df.loc[index , 'OUTPUTs'].split(';') )\n",
    "        #for tx_output in outputs_list:\n",
    "        #    address_raw        = tx_output.split(',')[1]\n",
    "        #    address_has_script = tx_output.split(',')[4]\n",
    "        #    payment_cred       = tx_output.split(',')[5]\n",
    "        #    stake_address      = tx_output.split(',')[6]\n",
    "        #    [address_payment_part, address_delegation_part] = extract_payment_delegation_parts(address_raw, payment_cred, stake_address)\n",
    "        #    if (address_payment_part != '' and address_delegation_part != ''):\n",
    "        #        indx1 = BinarySearch(unique_delegation_addresses, address_delegation_part)\n",
    "        #        indx2 = BinarySearch(unique_payment_addresses, address_payment_part)\n",
    "        #        stake_delegation_array[indx1].append(indx2)\n",
    "        ##########################################################################################\n",
    "\n",
    "        # key: entity_indx | value: TX_vol\n",
    "        TX_vol_dict = {}\n",
    "        \n",
    "        for tx_input in inputs_list:\n",
    "            tx_input_split = tx_input.split(',')\n",
    "            address_has_script = tx_input_split[7]\n",
    "            if (address_has_script == 'f'): # non-Smart Contract Address\n",
    "                address_raw   = tx_input_split[4]\n",
    "                payment_cred  = tx_input_split[8]\n",
    "                stake_address = tx_input_split[9]\n",
    "                [address_payment_part, address_delegation_part] = extract_payment_delegation_parts(address_raw, payment_cred, stake_address)\n",
    "                if (address_payment_part != ''):\n",
    "                    addr_indx   = BinarySearch(unique_payment_addresses, address_payment_part)\n",
    "                    entity_indx = clustering_array[addr_indx][0]\n",
    "                    UTXO_value  = int(tx_input_split[6])\n",
    "                    #balances_per_entity_array[entity_indx] = balances_per_entity_array[entity_indx] - int(UTXO_value)\n",
    "                    TX_vol_dict[entity_indx] = TX_vol_dict.get(entity_indx, 0) - UTXO_value\n",
    "\n",
    "        for tx_output in outputs_list:\n",
    "            tx_output_split = tx_output.split(',')\n",
    "            address_has_script = tx_output_split[4]\n",
    "            if (address_has_script == 'f'): # non-Smart Contract Address\n",
    "                address_raw   = tx_output_split[1]\n",
    "                payment_cred  = tx_output_split[5]\n",
    "                stake_address = tx_output_split[6]\n",
    "                [address_payment_part, address_delegation_part] = extract_payment_delegation_parts(address_raw, payment_cred, stake_address)\n",
    "                if (address_payment_part != ''):\n",
    "                    addr_indx   = BinarySearch(unique_payment_addresses, address_payment_part)\n",
    "                    entity_indx = clustering_array[addr_indx][0]\n",
    "                    UTXO_value  = int(tx_output_split[3])\n",
    "                    #balances_per_entity_array[entity_indx] = balances_per_entity_array[entity_indx] + int(UTXO_value)\n",
    "                    TX_vol_dict[entity_indx] = TX_vol_dict.get(entity_indx, 0) + UTXO_value\n",
    "\n",
    "        for key in TX_vol_dict:\n",
    "            TX_vol_per_entity_array[key] = TX_vol_per_entity_array[key] + abs(TX_vol_dict[key])\n",
    "\n",
    "\n",
    "        ##########################################################################################\n",
    "\n",
    "    et_temp = datetime.datetime.now() - ct_temp\n",
    "    print(\"elapsed time (Entities TX_vol from CSV File \" + file_name + \"): \", et_temp)\n",
    "\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "et = datetime.datetime.now() - ct\n",
    "print(\"Total elapsed time (Entities TX_vol): \", et)\n",
    "\n",
    "\n",
    "print('----------------------')\n",
    "print('done!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dd430-13e2-4e68-9593-35241e812b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb92a69-5480-4fd8-8e38-7df382353309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d0175a-b27c-4bc9-a8a5-65559fb523d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15682c92-40c6-4350-926b-730d792bd54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b464c7-096e-4285-a7a1-19544b7139e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3734cda6-2c27-4563-bfa4-35de57d46009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff8d8a-9a6c-4191-bf4a-c6d61c30beb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac736b-29a5-4a8a-8ceb-91dde6c6a824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
